{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0277080b-6129-484f-b56d-9181e0471626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e5e53-4ca3-4829-8deb-f3ce04b4818d",
   "metadata": {},
   "source": [
    "## Shannon Entropy\n",
    "\n",
    "The Shannon entropy is a statistical quantifier extensively used for the characterization of complex systems. It can be interpreted as:\n",
    "\n",
    "- __Measure of Uncertainty:__ It quantifies the unpredictability of information content. Higher entropy indicates greater uncertainty or variability in the outcomes of a random variable.\n",
    "- __Information Content:__ It represents the average number of bits needed to encode messages from a source. A source with uniform probability distribution (where all outcomes are equally likely) has maximum entropy, while a deterministic source (where one outcome is certain) has zero entropy.\n",
    "\n",
    "$$H(s) = -\\sum_{i=1} P_i \\log_2 P_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2080d-ae15-4d92-867e-7ecac78c07ce",
   "metadata": {},
   "source": [
    "When observed over time, the entropy is frequently used for anomalies detection. Expressive variations in the entropy $H(s)$ levels of a system can indicate a significant change in the system itself.\n",
    "$$\\Delta H(s) = H(s)_{t+1} - H(s)_{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bed2a0c-aafe-435a-8e56-8c6c523893f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_entropy(msg: str, base: int = 2) -> Tuple:\n",
    "    \"\"\"Calculates the Shannon entropy of a string message.\"\"\"\n",
    "    add = 0\n",
    "    symbols = {}\n",
    "    n = len(msg)\n",
    "    chars = set(list(msg))\n",
    "    for char in chars:\n",
    "        proba = msg.count(char) / n\n",
    "        add += proba * math.log(proba, base)\n",
    "        symbols[char] = proba\n",
    "    return add * -1, symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e0ed25-c5dc-43aa-b06d-ddbd4552d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 2.4464\n",
      "Symbols: {'e': 0.1, 'f': 0.1, 's': 0.3, 'c': 0.2, 'u': 0.2, 'l': 0.1}\n"
     ]
    }
   ],
   "source": [
    "h, symbols = message_entropy(msg=\"successful\", base=2)\n",
    "print(f\"Entropy: {h:.4f}\\nSymbols: {symbols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373bf04c-fc72-45eb-86c3-bd6b920566ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.9464\n",
      "Symbols: {'e': 0.1, 'f': 0.1, 's': 0.3, 'c': 0.2, 'u': 0.2, 'l': 0.1}\n"
     ]
    }
   ],
   "source": [
    "h, symbols = message_entropy(msg=\"successful\", base=6)\n",
    "print(f\"Entropy: {h:.4f}\\nSymbols: {symbols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad6f51a-ea9e-493c-9758-9d6469377e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 1.9219\n",
      "Symbols: {'E': 0.2, 'L': 0.4, 'O': 0.2, 'H': 0.2}\n"
     ]
    }
   ],
   "source": [
    "h, symbols = message_entropy(msg=\"HELLO\", base=2)\n",
    "print(f\"Entropy: {h:.4f}\\nSymbols: {symbols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92108790-cce3-4b51-ae74-e3a1b0d67fbc",
   "metadata": {},
   "source": [
    "- 1.92 (~ 2) bits needed for encode each symbol in the message.\n",
    "\n",
    "| Symbol | Code |\n",
    "|--------|------|\n",
    "| H      | 00   |\n",
    "| E      | 01   |\n",
    "| L      | 10   |\n",
    "| O      | 11   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72b572-f085-4842-87e9-79f7053ee69d",
   "metadata": {},
   "source": [
    "## Spacial Entropy\n",
    "\n",
    "Moreover, as proposed by Von Neumann, the Shannon entropy can be used to describe the spacial etropy and thus serving as a criterion for choosing spaces.\n",
    "\n",
    "Using normalized eigenvalues from Principal Component Analysis (PCA) as probabilities to estimate the entropy of a data space involves several key steps. This technique leverages the relationship between eigenvalues, variance, and information content in datasets.\n",
    "\n",
    "The spacial entropy value provides insight into the complexity or disorder within the dataset. A higher entropy indicates a more complex structure with less predictability, while lower entropy suggests a more ordered and predictable structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
